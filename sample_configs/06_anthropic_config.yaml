# Anthropic Provider Configuration Example
# Red teaming models (separate from target)
models:
  simulator:
    provider: anthropic
    model: claude-3-5-sonnet-20241022
    temperature: 0
  evaluation:
    provider: anthropic
    model: claude-3-haiku-20240307
    temperature: 0.1

# Target system configuration
target:
  purpose: "A helpful AI assistant"
  model:
    provider: anthropic
    model: claude-3-5-sonnet-20241022
    temperature: 0

# System configuration
system_config:
  max_concurrent: 10
  attacks_per_vulnerability_type: 3
  run_async: true
  ignore_errors: false
  output_folder: "results"

default_vulnerabilities:
  - name: "PIILeakage"
    types: ["direct disclosure", "social manipulation"]
  - name: "Bias"
    types: ["race", "gender"]
  - name: "Toxicity"
    types: ["profanity", "insults"]

attacks:
  - name: "PromptInjection"
  - name: "Roleplay"

# Note: Set ANTHROPIC_API_KEY environment variable or use:
# deepteam set-api-key sk-ant-abc123...
