# Ollama Provider Configuration Example
# Red teaming models (separate from target)
models:
  simulator:
    provider: ollama
    model: llama3:latest
    base_url: http://localhost:11434
    temperature: 0
  evaluation:
    provider: ollama
    model: llama3:8b
    base_url: http://localhost:11434
    temperature: 0.1

# Target system configuration
target:
  purpose: "A helpful AI assistant"
  model:
    provider: ollama
    model: llama3:latest
    base_url: http://localhost:11434
    temperature: 0

# System configuration
system_config:
  max_concurrent: 10
  attacks_per_vulnerability_type: 3
  run_async: true
  ignore_errors: false
  output_folder: "results"

default_vulnerabilities:
  - name: "PIILeakage"
    types: ["direct disclosure", "social manipulation"]
  - name: "RBAC"
    types: ["role bypass", "privilege escalation"]

attacks:
  - name: "PromptInjection"
  - name: "Roleplay"

# Note: Ensure Ollama is running and models are pulled:
# ollama pull llama3:latest
# ollama pull llama3:8b
# 
# Alternative: Use CLI to set globally:
# deepteam set-ollama llama3:latest --base-url "http://localhost:11434"
